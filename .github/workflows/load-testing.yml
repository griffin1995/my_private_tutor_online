# CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - GitHub Actions integration for automated performance testing
# IMPLEMENTATION REASON: Official Lighthouse CI pattern for continuous performance monitoring in CI/CD

name: FAQ Load Testing & Performance Monitoring

on:
  push:
    branches: [ master, develop ]
    paths:
      - 'src/components/faq/**'
      - 'src/lib/faq/**'
      - 'src/app/faq/**'
      - 'load-tests/**'
      - '.github/workflows/load-testing.yml'
  pull_request:
    branches: [ master ]
    paths:
      - 'src/components/faq/**'
      - 'src/lib/faq/**'
      - 'src/app/faq/**'
  schedule:
    # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Scheduled performance monitoring
    # SCHEDULE REASON: Regular performance monitoring for royal client service standards
    - cron: '0 */6 * * *'  # Every 6 hours
    - cron: '0 9 * * 1-5'  # Weekdays at 9 AM (peak usage simulation)
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of load test to run'
        required: true
        default: 'baseline'
        type: choice
        options:
          - 'baseline'
          - 'royal-peak'
          - 'stress-breaking-point'
          - 'accessibility'
          - 'all'
      target_url:
        description: 'Target URL for testing'
        required: false
        default: 'https://myprivatetutoronline-991oq6we4-jacks-projects-cf5effed.vercel.app'

# CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Environment variables for performance testing
# ENVIRONMENT REASON: Secure configuration management for load testing infrastructure
env:
  NODE_VERSION: '18'
  BASE_URL: ${{ github.event.inputs.target_url || 'https://myprivatetutoronline-991oq6we4-jacks-projects-cf5effed.vercel.app' }}
  TEST_RESULTS_RETENTION: 30  # Days to retain test results
  
jobs:
  # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Performance prerequisites validation
  # VALIDATION REASON: Ensure target environment is ready for load testing
  validate-environment:
    name: Validate FAQ System Environment
    runs-on: ubuntu-latest
    outputs:
      environment-ready: ${{ steps.health-check.outputs.ready }}
      baseline-performance: ${{ steps.performance-check.outputs.baseline }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Environment health validation
      # HEALTH CHECK REASON: Verify FAQ system availability before load testing
      - name: FAQ System Health Check
        id: health-check
        run: |
          echo "Validating FAQ system availability at $BASE_URL"
          
          # Check main FAQ endpoint
          response=$(curl -s -w "%{http_code}" -o /dev/null "$BASE_URL/faq" || echo "000")
          if [ "$response" != "200" ]; then
            echo "âŒ FAQ homepage not accessible (HTTP $response)"
            echo "ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check FAQ API endpoints
          api_response=$(curl -s -w "%{http_code}" -o /dev/null "$BASE_URL/api/faq/health" || echo "000")
          if [ "$api_response" != "200" ]; then
            echo "âŒ FAQ API not accessible (HTTP $api_response)"
            echo "ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "âœ… FAQ system health check passed"
          echo "ready=true" >> $GITHUB_OUTPUT
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Baseline performance measurement
      # BASELINE REASON: Establish performance baseline before load testing
      - name: Baseline Performance Check
        id: performance-check
        run: |
          echo "Measuring baseline FAQ performance"
          
          # Install lighthouse for baseline measurement
          npm install -g lighthouse@latest
          
          # Run baseline lighthouse audit
          lighthouse "$BASE_URL/faq" \
            --output=json \
            --output-path=./baseline-performance.json \
            --chrome-flags="--headless --no-sandbox --disable-gpu" \
            --preset=desktop \
            --throttling-method=provided
          
          # Extract key metrics
          fcp=$(cat baseline-performance.json | jq -r '.audits["first-contentful-paint"].numericValue')
          lcp=$(cat baseline-performance.json | jq -r '.audits["largest-contentful-paint"].numericValue')
          performance_score=$(cat baseline-performance.json | jq -r '.categories.performance.score')
          
          echo "ðŸ“Š Baseline Performance Metrics:"
          echo "   First Contentful Paint: ${fcp}ms"
          echo "   Largest Contentful Paint: ${lcp}ms"
          echo "   Performance Score: $(echo "$performance_score * 100" | bc)%"
          
          echo "baseline={\"fcp\":$fcp,\"lcp\":$lcp,\"score\":$performance_score}" >> $GITHUB_OUTPUT
      
      - name: Upload Baseline Results
        uses: actions/upload-artifact@v4
        with:
          name: baseline-performance-${{ github.run_id }}
          path: baseline-performance.json
          retention-days: ${{ env.TEST_RESULTS_RETENTION }}

  # CONTEXT7 SOURCE: /grafana/k6-docs - K6 load testing job configuration
  # K6 TESTING REASON: Comprehensive load testing with k6 for FAQ system validation
  k6-load-testing:
    name: K6 Load Testing
    runs-on: ubuntu-latest
    needs: validate-environment
    if: needs.validate-environment.outputs.environment-ready == 'true'
    
    strategy:
      # CONTEXT7 SOURCE: /grafana/k6-docs - Matrix testing for comprehensive coverage
      # MATRIX REASON: Test multiple FAQ scenarios in parallel for efficiency
      matrix:
        test-scenario:
          - name: 'baseline'
            file: 'faq-baseline-load.js'
            description: 'FAQ Baseline Load Test'
          - name: 'royal-peak'
            file: 'faq-royal-peak-load.js'
            description: 'Royal Client Peak Load Test'
          - name: 'stress-breaking-point'
            file: 'faq-stress-breaking-point.js'
            description: 'FAQ Stress Breaking Point Test'
          - name: 'accessibility'
            file: 'faq-accessibility-load.js'
            description: 'FAQ Accessibility Load Test'
      
      fail-fast: false  # Continue other tests if one fails
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      # CONTEXT7 SOURCE: /grafana/k6-docs - K6 installation and setup
      # SETUP REASON: Official k6 installation pattern for GitHub Actions
      - name: Install K6
        run: |
          echo "Installing K6 load testing tool"
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
          k6 version
      
      # Skip specific test if requested
      - name: Check Test Filter
        id: test-filter
        run: |
          if [[ "${{ github.event.inputs.test_type }}" != "all" && "${{ github.event.inputs.test_type }}" != "${{ matrix.test-scenario.name }}" ]]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "Skipping ${{ matrix.test-scenario.name }} test (requested: ${{ github.event.inputs.test_type }})"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
            echo "Running ${{ matrix.test-scenario.name }} test"
          fi
      
      # CONTEXT7 SOURCE: /grafana/k6-docs - K6 test execution with environment configuration
      # EXECUTION REASON: Run FAQ load tests with proper environment and result collection
      - name: Execute K6 Load Test - ${{ matrix.test-scenario.description }}
        if: steps.test-filter.outputs.skip == 'false'
        run: |
          echo "ðŸš€ Starting ${{ matrix.test-scenario.description }}"
          echo "Target URL: $BASE_URL"
          
          # Set test-specific environment variables
          export BASE_URL="${BASE_URL}"
          export TEST_NAME="${{ matrix.test-scenario.name }}"
          export GITHUB_RUN_ID="${{ github.run_id }}"
          export BRANCH_NAME="${{ github.ref_name }}"
          
          # Configure test parameters based on scenario
          case "${{ matrix.test-scenario.name }}" in
            "baseline")
              export TARGET_VUS=500
              export RAMP_DURATION="2m"
              export PLATEAU_DURATION="10m"
              ;;
            "royal-peak")
              export ROYAL_CLIENT_RATIO=0.3
              export PEAK_ARRIVAL_RATE=100
              export EXAM_SEASON_MULTIPLIER=2.5
              ;;
            "stress-breaking-point")
              export INITIAL_LOAD=50
              export MAX_EXPECTED_LOAD=2000
              export LOAD_INCREMENT=50
              ;;
            "accessibility")
              export SCREEN_READER_RATIO=0.15
              export KEYBOARD_ONLY_RATIO=0.25
              export HIGH_CONTRAST_RATIO=0.20
              export VOICE_CONTROL_RATIO=0.10
              ;;
          esac
          
          # Run K6 test with timeout protection
          timeout 30m k6 run \
            --out json=./k6-results-${{ matrix.test-scenario.name }}.json \
            ./load-tests/k6/${{ matrix.test-scenario.file }}
          
          echo "âœ… ${{ matrix.test-scenario.description }} completed"
      
      # CONTEXT7 SOURCE: /grafana/k6-docs - Test results processing and analysis
      # RESULTS REASON: Process k6 results for GitHub Actions reporting and artifact storage
      - name: Process Test Results
        if: steps.test-filter.outputs.skip == 'false' && (success() || failure())
        run: |
          echo "ðŸ“Š Processing ${{ matrix.test-scenario.name }} test results"
          
          # Check if results file exists
          if [[ ! -f "./k6-results-${{ matrix.test-scenario.name }}.json" ]]; then
            echo "âš ï¸ No results file found for ${{ matrix.test-scenario.name }}"
            exit 0
          fi
          
          # Extract key metrics from k6 results
          results_file="./k6-results-${{ matrix.test-scenario.name }}.json"
          
          # Calculate summary metrics
          echo "## ${{ matrix.test-scenario.description }} Results" >> test-summary.md
          echo "" >> test-summary.md
          
          # Add test-specific metrics processing
          if [[ -f "./${{ matrix.test-scenario.name }}-results.json" ]]; then
            echo "### Key Performance Metrics" >> test-summary.md
            echo '```json' >> test-summary.md
            cat "./${{ matrix.test-scenario.name }}-results.json" >> test-summary.md
            echo '```' >> test-summary.md
          fi
          
          echo "" >> test-summary.md
      
      - name: Upload Test Results
        if: steps.test-filter.outputs.skip == 'false' && (success() || failure())
        uses: actions/upload-artifact@v4
        with:
          name: k6-results-${{ matrix.test-scenario.name }}-${{ github.run_id }}
          path: |
            ./k6-results-${{ matrix.test-scenario.name }}.json
            ./${{ matrix.test-scenario.name }}-results.json
            ./test-summary.md
          retention-days: ${{ env.TEST_RESULTS_RETENTION }}

  # CONTEXT7 SOURCE: Artillery.js - Complex user journey testing
  # ARTILLERY REASON: Artillery.js specializes in complex user workflow simulation
  artillery-user-journeys:
    name: Artillery Complex User Journeys
    runs-on: ubuntu-latest
    needs: validate-environment
    if: needs.validate-environment.outputs.environment-ready == 'true' && (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'user-journeys')
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      # CONTEXT7 SOURCE: Artillery.js - Artillery installation and setup
      # INSTALLATION REASON: Official Artillery.js setup for complex user journey testing
      - name: Install Artillery
        run: |
          echo "Installing Artillery for complex user journey testing"
          npm install -g artillery@latest
          artillery --version
      
      # Prepare test data
      - name: Generate Test Data
        run: |
          echo "Generating FAQ test data for complex user journeys"
          
          # Create FAQ search terms data
          mkdir -p ./load-tests/artillery/data
          
          # Generate search terms CSV
          cat > ./load-tests/artillery/data/faq-search-terms.csv << EOF
          search_term,category,user_type,expected_results
          tutoring rates,pricing,standard,5
          Oxbridge preparation,education,royal,10
          11+ examination help,education,standard,8
          royal endorsed tutors,elite-services,royal,3
          accessibility features,support,accessibility,6
          keyboard navigation,support,accessibility,4
          voice control help,support,accessibility,5
          booking process,general,standard,7
          cancellation policy,policies,standard,3
          confidential tutoring,privacy,royal,4
          EOF
          
          # Generate royal client queries CSV
          cat > ./load-tests/artillery/data/royal-client-queries.csv << EOF
          royal_query,service_tier,urgency_level,confidentiality_required
          Cambridge entrance coaching,premium,high,true
          Oxford interview preparation,premium,urgent,true
          Eton preparation programme,elite,high,true
          diplomatic family tutoring,elite,medium,true
          international royal services,premium,standard,true
          bespoke education packages,elite,high,true
          confidential elite coaching,premium,urgent,true
          royal protocol tutoring,elite,high,true
          EOF
      
      # CONTEXT7 SOURCE: Artillery.js - Complex user journey execution
      # EXECUTION REASON: Run Artillery.js tests for comprehensive user workflow validation
      - name: Execute Artillery User Journey Tests
        run: |
          echo "ðŸŽ­ Starting Artillery Complex User Journey Tests"
          echo "Target: $BASE_URL"
          
          # Set environment variables for Artillery
          export BASE_URL="${BASE_URL}"
          export TEST_ENVIRONMENT="github-actions"
          export GITHUB_RUN_ID="${{ github.run_id }}"
          
          # Run Artillery test with JSON output
          artillery run \
            --output ./artillery-user-journeys-results.json \
            ./load-tests/artillery/faq-complex-user-journeys.yml
          
          echo "âœ… Artillery user journey tests completed"
      
      - name: Process Artillery Results
        if: success() || failure()
        run: |
          echo "ðŸ“ˆ Processing Artillery test results"
          
          if [[ -f "./artillery-user-journeys-results.json" ]]; then
            echo "## Artillery Complex User Journeys Results" >> artillery-summary.md
            echo "" >> artillery-summary.md
            
            # Extract key metrics (Artillery results are different from k6)
            echo "### Test Execution Summary" >> artillery-summary.md
            echo "- Test completed at: $(date)" >> artillery-summary.md
            echo "- Target URL: $BASE_URL" >> artillery-summary.md
            echo "- Test Configuration: Complex User Journeys" >> artillery-summary.md
            echo "" >> artillery-summary.md
            
            echo "âœ… Artillery results processed"
          else
            echo "âš ï¸ Artillery results file not found"
          fi
      
      - name: Upload Artillery Results
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: artillery-user-journeys-${{ github.run_id }}
          path: |
            ./artillery-user-journeys-results.json
            ./artillery-summary.md
          retention-days: ${{ env.TEST_RESULTS_RETENTION }}

  # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Lighthouse CI performance validation
  # LIGHTHOUSE REASON: Validate FAQ performance meets Core Web Vitals and accessibility standards
  lighthouse-performance-audit:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    needs: validate-environment
    if: needs.validate-environment.outputs.environment-ready == 'true'
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Lighthouse CI installation
      # INSTALLATION REASON: Official Lighthouse CI setup for automated performance monitoring
      - name: Install Lighthouse CI
        run: |
          echo "Installing Lighthouse CI for FAQ performance auditing"
          npm install -g @lhci/cli@0.15.x
          lhci --version
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Performance budget configuration
      # BUDGET REASON: Enforce performance standards for royal client service requirements
      - name: Create Performance Budget
        run: |
          echo "Creating performance budget for FAQ system"
          
          cat > ./budget.json << EOF
          [
            {
              "resourceSizes": [
                {
                  "resourceType": "document",
                  "budget": 50
                },
                {
                  "resourceType": "script",
                  "budget": 150
                },
                {
                  "resourceType": "stylesheet", 
                  "budget": 25
                },
                {
                  "resourceType": "image",
                  "budget": 200
                },
                {
                  "resourceType": "total",
                  "budget": 500
                }
              ],
              "resourceCounts": [
                {
                  "resourceType": "third-party",
                  "budget": 10
                }
              ],
              "timings": [
                {
                  "metric": "first-contentful-paint",
                  "budget": 1500
                },
                {
                  "metric": "largest-contentful-paint", 
                  "budget": 2500
                },
                {
                  "metric": "cumulative-layout-shift",
                  "budget": 0.1
                },
                {
                  "metric": "interactive",
                  "budget": 3000
                }
              ]
            }
          ]
          EOF
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Lighthouse configuration for FAQ testing
      # CONFIGURATION REASON: Customized Lighthouse setup for FAQ accessibility and performance
      - name: Create Lighthouse Configuration
        run: |
          cat > ./lighthouserc.json << EOF
          {
            "ci": {
              "collect": {
                "numberOfRuns": 3,
                "settings": {
                  "budgetPath": "./budget.json",
                  "preset": "desktop",
                  "chromeFlags": "--no-sandbox --headless --disable-gpu",
                  "skipAudits": ["redirects-http"]
                },
                "url": [
                  "$BASE_URL/faq",
                  "$BASE_URL/faq?accessibility=enhanced",
                  "$BASE_URL/faq?tier=royal"
                ]
              },
              "assert": {
                "preset": "lighthouse:recommended",
                "assertions": {
                  "categories:performance": ["error", {"minScore": 0.9}],
                  "categories:accessibility": ["error", {"minScore": 0.95}],
                  "categories:best-practices": ["error", {"minScore": 0.9}],
                  "categories:seo": ["error", {"minScore": 0.9}],
                  "first-contentful-paint": ["error", {"maxNumericValue": 1500}],
                  "largest-contentful-paint": ["error", {"maxNumericValue": 2500}],
                  "cumulative-layout-shift": ["error", {"maxNumericValue": 0.1}],
                  "interactive": ["error", {"maxNumericValue": 3000}],
                  "color-contrast": "error",
                  "heading-order": "error",
                  "aria-valid-attr": "error"
                }
              },
              "upload": {
                "target": "temporary-public-storage"
              }
            }
          }
          EOF
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Lighthouse CI execution
      # EXECUTION REASON: Run comprehensive FAQ performance and accessibility audits
      - name: Run Lighthouse CI Audits
        run: |
          echo "ðŸ” Running Lighthouse CI audits for FAQ system"
          echo "Testing URLs:"
          echo "  - $BASE_URL/faq (Standard FAQ)"
          echo "  - $BASE_URL/faq?accessibility=enhanced (Accessibility Mode)"
          echo "  - $BASE_URL/faq?tier=royal (Royal Client Mode)"
          
          # Run Lighthouse CI with error handling
          lhci autorun || {
            echo "âŒ Lighthouse CI failed - checking for results"
            ls -la .lighthouseci/ || echo "No .lighthouseci directory found"
            exit 1
          }
          
          echo "âœ… Lighthouse CI audits completed"
      
      - name: Upload Lighthouse Results
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-ci-results-${{ github.run_id }}
          path: |
            .lighthouseci/
            ./budget.json
            ./lighthouserc.json
          retention-days: ${{ env.TEST_RESULTS_RETENTION }}

  # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Performance monitoring summary
  # SUMMARY REASON: Consolidate all performance test results for comprehensive reporting
  performance-summary:
    name: Performance Test Summary
    runs-on: ubuntu-latest
    needs: [validate-environment, k6-load-testing, artillery-user-journeys, lighthouse-performance-audit]
    if: always() && needs.validate-environment.outputs.environment-ready == 'true'
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      # Download all test artifacts
      - name: Download Test Results
        uses: actions/download-artifact@v4
        with:
          path: ./test-results/
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Comprehensive performance reporting
      # REPORTING REASON: Generate executive summary of FAQ performance testing results
      - name: Generate Performance Summary Report
        run: |
          echo "ðŸ“Š Generating comprehensive FAQ performance summary"
          
          # Create summary report
          cat > ./performance-summary.md << EOF
          # FAQ System Performance Test Results
          
          **Test Run ID:** ${{ github.run_id }}  
          **Branch:** ${{ github.ref_name }}  
          **Target URL:** $BASE_URL  
          **Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          
          ## Test Execution Summary
          
          | Test Suite | Status | Results Available |
          |------------|--------|-------------------|
          EOF
          
          # Check K6 test results
          for scenario in baseline royal-peak stress-breaking-point accessibility; do
            if [[ -d "./test-results/k6-results-${scenario}-${{ github.run_id }}" ]]; then
              echo "| K6 ${scenario^} Load Test | âœ… Completed | Yes |" >> ./performance-summary.md
            else
              echo "| K6 ${scenario^} Load Test | âŒ Failed/Skipped | No |" >> ./performance-summary.md
            fi
          done
          
          # Check Artillery results
          if [[ -d "./test-results/artillery-user-journeys-${{ github.run_id }}" ]]; then
            echo "| Artillery User Journeys | âœ… Completed | Yes |" >> ./performance-summary.md
          else
            echo "| Artillery User Journeys | âŒ Failed/Skipped | No |" >> ./performance-summary.md
          fi
          
          # Check Lighthouse results
          if [[ -d "./test-results/lighthouse-ci-results-${{ github.run_id }}" ]]; then
            echo "| Lighthouse CI Performance | âœ… Completed | Yes |" >> ./performance-summary.md
          else
            echo "| Lighthouse CI Performance | âŒ Failed/Skipped | No |" >> ./performance-summary.md
          fi
          
          cat >> ./performance-summary.md << EOF
          
          ## Royal Client Service Standards Compliance
          
          The FAQ system has been tested against our royal client service standards:
          
          - **Performance Target**: 95% of requests under 100ms â±ï¸
          - **Availability Target**: 99.99% uptime reliability ðŸ”’
          - **Accessibility Standard**: WCAG 2.1 AA compliance â™¿
          - **Royal Client SLA**: Sub-50ms response times for premium features ðŸ‘‘
          
          ## Test Results Analysis
          
          Detailed test results are available in the GitHub Actions artifacts. Each test suite provides specific metrics and recommendations for FAQ system optimization.
          
          ### Next Steps
          
          1. Review individual test results for performance bottlenecks
          2. Implement recommended optimizations
          3. Validate fixes with follow-up testing
          4. Monitor production performance continuously
          
          ---
          
          *Generated by FAQ Load Testing & Performance Monitoring Pipeline*  
          *My Private Tutor Online - Royal Client Service Excellence*
          EOF
          
          echo "âœ… Performance summary report generated"
      
      - name: Upload Performance Summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary-${{ github.run_id }}
          path: ./performance-summary.md
          retention-days: ${{ env.TEST_RESULTS_RETENTION }}
      
      # CONTEXT7 SOURCE: /googlechrome/lighthouse-ci - Pull request comment integration
      # COMMENT REASON: Provide immediate performance feedback on pull requests
      - name: Comment Performance Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const summary = fs.readFileSync('./performance-summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸ“Š FAQ Performance Test Results\n\n${summary}\n\n**ðŸ”— Detailed Results:** Check the [GitHub Actions artifacts](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for comprehensive test data.`
              });
              
              console.log('âœ… Performance results posted to PR');
            } catch (error) {
              console.error('âŒ Failed to post PR comment:', error.message);
            }